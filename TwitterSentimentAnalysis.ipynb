{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5780ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6781c06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('dataset.csv', on_bad_lines='skip')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8db9be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1999 entries, 0 to 1998\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   ItemID           1999 non-null   int64 \n",
      " 1   Sentiment        1999 non-null   int64 \n",
      " 2   SentimentSource  1999 non-null   object\n",
      " 3   SentimentText    1999 non-null   object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 62.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()\n",
    "data.replace({\"positive\":1,\"negative\":0},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1cdaa10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Review_without_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \\\n",
       "0                       is so sad for my APL frie...   \n",
       "1                     I missed the New Moon trail...   \n",
       "2                            omg its already 7:30 :O   \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "4           i think mi bf is cheating on me!!!   ...   \n",
       "\n",
       "                            Review_without_stopwords  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Edited_Review = data['SentimentText'].copy()\n",
    "data['Review_without_stopwords'] = Edited_Review\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e89c53ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess tweets \n",
    "def preprocess_tweet_data(data,name):\n",
    "    # Proprocessing the data\n",
    "    data[name]=data[name].str.lower()\n",
    "    # Code to remove the Hashtags from the text\n",
    "    data[name]=data[name].apply(lambda x:re.sub(r'\\B#\\S+','',x))\n",
    "    # Code to remove the links from the text\n",
    "    data[name]=data[name].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "    # Code to remove the Special characters from the text \n",
    "    data[name]=data[name].apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n",
    "    # Code to substitute the multiple spaces with single spaces\n",
    "    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n",
    "    # Code to remove all the single characters in the text\n",
    "    data[name]=data[name].apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n",
    "    # Remove the twitter handlers\n",
    "    data[name]=data[name].apply(lambda x:re.sub('@[^\\s]+','',x))\n",
    "\n",
    "# Function to tokenize and remove the stopwords    \n",
    "def rem_stopwords_tokenize(data,name):\n",
    "      \n",
    "    def getting(sen):\n",
    "        example_sent = sen\n",
    "        \n",
    "        filtered_sentence = [] \n",
    "\n",
    "        stop_words = set(stopwords.words('english')) \n",
    "\n",
    "        word_tokens = word_tokenize(example_sent) \n",
    "        \n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "        \n",
    "        return filtered_sentence\n",
    "    # Using \"getting(sen)\" function to append edited sentence to data\n",
    "    x=[]\n",
    "    for i in data[name].values:\n",
    "        x.append(getting(i))\n",
    "    data[name]=x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7828931",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def Lemmatization(data,name):\n",
    "    def getting2(sen):\n",
    "        \n",
    "        example = sen\n",
    "        output_sentence =[]\n",
    "        word_tokens2 = word_tokenize(example)\n",
    "        lemmatized_output = [lemmatizer.lemmatize(w) for w in word_tokens2]\n",
    "        \n",
    "        # Remove characters which have length less than 2  \n",
    "        without_single_chr = [word for word in lemmatized_output if len(word) > 2]\n",
    "        # Remove numbers\n",
    "        cleaned_data_title = [word for word in without_single_chr if not word.isnumeric()]\n",
    "        \n",
    "        return cleaned_data_title\n",
    "    # Using \"getting2(sen)\" function to append edited sentence to data\n",
    "    x=[]\n",
    "    for i in data[name].values:\n",
    "        x.append(getting2(i))\n",
    "    data[name]=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fefc0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentences(data,name):\n",
    "    data[name]=data[name].apply(lambda x:' '.join([i+' ' for i in x]))\n",
    "    # Removing double spaces if created\n",
    "    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d67a517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\18604\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ddedf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the preprocessing function to preprocess the tweets\n",
    "preprocess_tweet_data(data,'Review_without_stopwords')\n",
    "# Using tokenizer and removing the stopwords\n",
    "rem_stopwords_tokenize(data,'Review_without_stopwords')\n",
    "# Converting all the texts back to sentences\n",
    "make_sentences(data,'Review_without_stopwords')\n",
    "\n",
    "#Edits After Lemmatization\n",
    "final_Edit = data['Review_without_stopwords'].copy()\n",
    "data[\"After_lemmatization\"] = final_Edit\n",
    "\n",
    "# Using the Lemmatization function to lemmatize the hotel data\n",
    "Lemmatization(data,'After_lemmatization')\n",
    "# Converting all the texts back to sentences\n",
    "make_sentences(data,'After_lemmatization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1671b41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Review_without_stopwords</th>\n",
       "      <th>After_lemmatization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>sad apl friend</td>\n",
       "      <td>sad apl friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>missed new moon trailer</td>\n",
       "      <td>missed new moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>omg already 7 30</td>\n",
       "      <td>omg already</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>omgaga im sooo im gunna cryve dentist since 11...</td>\n",
       "      <td>omgaga sooo gunna cryve dentist since 11was su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>think mi bf cheating t_t</td>\n",
       "      <td>think cheating t_t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \\\n",
       "0                       is so sad for my APL frie...   \n",
       "1                     I missed the New Moon trail...   \n",
       "2                            omg its already 7:30 :O   \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "4           i think mi bf is cheating on me!!!   ...   \n",
       "\n",
       "                            Review_without_stopwords  \\\n",
       "0                                    sad apl friend    \n",
       "1                           missed new moon trailer    \n",
       "2                                  omg already 7 30    \n",
       "3  omgaga im sooo im gunna cryve dentist since 11...   \n",
       "4                          think mi bf cheating t_t    \n",
       "\n",
       "                                 After_lemmatization  \n",
       "0                                    sad apl friend   \n",
       "1                           missed new moon trailer   \n",
       "2                                       omg already   \n",
       "3  omgaga sooo gunna cryve dentist since 11was su...  \n",
       "4                                think cheating t_t   "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b7a0c5",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbb33d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cca7bdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scanning takes up a lot of my time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WandaMoosejaw I dont think I have Link  Maybe ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My hairs still wet Who cared its warm outside ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>timalmond Dont fret Ill hand mine in too  Im l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chiefbombp chrissinicole maqriche Oldies conce...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment\n",
       "0                Scanning takes up a lot of my time           0\n",
       "1  WandaMoosejaw I dont think I have Link  Maybe ...          1\n",
       "2  My hairs still wet Who cared its warm outside ...          1\n",
       "3  timalmond Dont fret Ill hand mine in too  Im l...          1\n",
       "4  chiefbombp chrissinicole maqriche Oldies conce...          1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining the function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "  if(type(text)==float):\n",
    "    return text\n",
    "  ans=\"\"  \n",
    "  for i in text:     \n",
    "    if i not in string.punctuation:\n",
    "      ans+=i    \n",
    "  return ans\n",
    "\n",
    "#storing the puntuation free text in a new column called clean_msg\n",
    "df_train['tweet']= df_train['tweet'].apply(lambda x:remove_punctuation(x))\n",
    "df_test['tweet']= df_test['tweet'].apply(lambda x:remove_punctuation(x))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ae594ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e9d0239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\18604\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62a19f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\18604\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\sentiwordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2c40a41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      -0.500\n",
      "1       0.375\n",
      "2       0.125\n",
      "3       0.000\n",
      "4       0.000\n",
      "        ...  \n",
      "1994    0.000\n",
      "1995    0.000\n",
      "1996    0.000\n",
      "1997    0.000\n",
      "1998    0.375\n",
      "Name: senti_score, Length: 1999, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Review_without_stopwords</th>\n",
       "      <th>After_lemmatization</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>senti_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>sad apl friend</td>\n",
       "      <td>sad apl friend</td>\n",
       "      <td>[(sad, JJ), (apl, NN), (friend, NN)]</td>\n",
       "      <td>-0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>missed new moon trailer</td>\n",
       "      <td>missed new moon trailer</td>\n",
       "      <td>[(missed, VBN), (new, JJ), (moon, NN), (traile...</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>omg already 7 30</td>\n",
       "      <td>omg already</td>\n",
       "      <td>[(omg, NNS), (already, RB)]</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>omgaga im sooo im gunna cryve dentist since 11...</td>\n",
       "      <td>omgaga sooo gunna cryve dentist since 11was su...</td>\n",
       "      <td>[(omgaga, JJ), (sooo, NN), (gunna, NN), (cryve...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>think mi bf cheating t_t</td>\n",
       "      <td>think cheating t_t</td>\n",
       "      <td>[(think, NN), (cheating, VBG), (t_t, NN)]</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \\\n",
       "0                       is so sad for my APL frie...   \n",
       "1                     I missed the New Moon trail...   \n",
       "2                            omg its already 7:30 :O   \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "4           i think mi bf is cheating on me!!!   ...   \n",
       "\n",
       "                            Review_without_stopwords  \\\n",
       "0                                    sad apl friend    \n",
       "1                           missed new moon trailer    \n",
       "2                                  omg already 7 30    \n",
       "3  omgaga im sooo im gunna cryve dentist since 11...   \n",
       "4                          think mi bf cheating t_t    \n",
       "\n",
       "                                 After_lemmatization  \\\n",
       "0                                    sad apl friend    \n",
       "1                           missed new moon trailer    \n",
       "2                                       omg already    \n",
       "3  omgaga sooo gunna cryve dentist since 11was su...   \n",
       "4                                think cheating t_t    \n",
       "\n",
       "                                            pos_tags  senti_score  \n",
       "0               [(sad, JJ), (apl, NN), (friend, NN)]       -0.500  \n",
       "1  [(missed, VBN), (new, JJ), (moon, NN), (traile...        0.375  \n",
       "2                        [(omg, NNS), (already, RB)]        0.125  \n",
       "3  [(omgaga, JJ), (sooo, NN), (gunna, NN), (cryve...        0.000  \n",
       "4          [(think, NN), (cheating, VBG), (t_t, NN)]        0.000  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos=neg=obj=count=0\n",
    "\n",
    "postagging = []\n",
    "\n",
    "for review in data['After_lemmatization']:\n",
    "    list = word_tokenize(review)\n",
    "    postagging.append(nltk.pos_tag(list))\n",
    "\n",
    "data['pos_tags'] = postagging\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "         return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "\n",
    "# Returns list of pos-neg and objective score. But returns empty list if not present in senti wordnet.\n",
    "def get_sentiment(word,tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    \n",
    "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "        return []\n",
    "\n",
    "    #Lemmatization\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "    if not lemma:\n",
    "        return []\n",
    "\n",
    "    #Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet. \n",
    "    #Synset instances are the groupings of synonymous words that express the same concept. \n",
    "    #Some of the words have only one Synset and some have several.\n",
    "    synsets = wn.synsets(word, pos=wn_tag)\n",
    "    if not synsets:\n",
    "        return []\n",
    "\n",
    "    # Take the first sense, the most common\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "    return [synset.name(), swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]\n",
    "\n",
    "    pos=neg=obj=count=0\n",
    "    \n",
    "    ###################################################################################\n",
    "senti_score = []\n",
    "\n",
    "for pos_val in data['pos_tags']:\n",
    "    senti_val = [get_sentiment(x,y) for (x,y) in pos_val]\n",
    "    for score in senti_val:\n",
    "        try:\n",
    "            pos = pos + score[1]  #positive score is stored at 2nd position\n",
    "            neg = neg + score[2]  #negative score is stored at 3rd position\n",
    "        except:\n",
    "            continue\n",
    "    senti_score.append(pos - neg)\n",
    "    pos=neg=0    \n",
    "data['senti_score'] = senti_score\n",
    "print(data['senti_score'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4dfa72d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall=[]\n",
    "for i in range(len(data)):\n",
    "    if data['senti_score'][i]>= 0.05:\n",
    "        overall.append('Positive')\n",
    "    elif data['senti_score'][i]<= -0.05:\n",
    "        overall.append('Negative')\n",
    "    else:\n",
    "        overall.append('Neutral')\n",
    "data['Overall Sentiment']=overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "528ab6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Review_without_stopwords</th>\n",
       "      <th>After_lemmatization</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>senti_score</th>\n",
       "      <th>Overall Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>sad apl friend</td>\n",
       "      <td>sad apl friend</td>\n",
       "      <td>[(sad, JJ), (apl, NN), (friend, NN)]</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>missed new moon trailer</td>\n",
       "      <td>missed new moon trailer</td>\n",
       "      <td>[(missed, VBN), (new, JJ), (moon, NN), (traile...</td>\n",
       "      <td>0.375</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>omg already 7 30</td>\n",
       "      <td>omg already</td>\n",
       "      <td>[(omg, NNS), (already, RB)]</td>\n",
       "      <td>0.125</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>omgaga im sooo im gunna cryve dentist since 11...</td>\n",
       "      <td>omgaga sooo gunna cryve dentist since 11was su...</td>\n",
       "      <td>[(omgaga, JJ), (sooo, NN), (gunna, NN), (cryve...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>think mi bf cheating t_t</td>\n",
       "      <td>think cheating t_t</td>\n",
       "      <td>[(think, NN), (cheating, VBG), (t_t, NN)]</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \\\n",
       "0                       is so sad for my APL frie...   \n",
       "1                     I missed the New Moon trail...   \n",
       "2                            omg its already 7:30 :O   \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "4           i think mi bf is cheating on me!!!   ...   \n",
       "\n",
       "                            Review_without_stopwords  \\\n",
       "0                                    sad apl friend    \n",
       "1                           missed new moon trailer    \n",
       "2                                  omg already 7 30    \n",
       "3  omgaga im sooo im gunna cryve dentist since 11...   \n",
       "4                          think mi bf cheating t_t    \n",
       "\n",
       "                                 After_lemmatization  \\\n",
       "0                                    sad apl friend    \n",
       "1                           missed new moon trailer    \n",
       "2                                       omg already    \n",
       "3  omgaga sooo gunna cryve dentist since 11was su...   \n",
       "4                                think cheating t_t    \n",
       "\n",
       "                                            pos_tags  senti_score  \\\n",
       "0               [(sad, JJ), (apl, NN), (friend, NN)]       -0.500   \n",
       "1  [(missed, VBN), (new, JJ), (moon, NN), (traile...        0.375   \n",
       "2                        [(omg, NNS), (already, RB)]        0.125   \n",
       "3  [(omgaga, JJ), (sooo, NN), (gunna, NN), (cryve...        0.000   \n",
       "4          [(think, NN), (cheating, VBG), (t_t, NN)]        0.000   \n",
       "\n",
       "  Overall Sentiment  \n",
       "0          Negative  \n",
       "1          Positive  \n",
       "2          Positive  \n",
       "3           Neutral  \n",
       "4           Neutral  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67312b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fe52f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file from gdrive\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data=pd.read_csv('cleaned_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8340e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb0ffbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5466 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(data['SentimentText'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "53f3c0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1999, 250)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = tokenizer.texts_to_sequences(data['SentimentText'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6dda834f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (1999,)\n"
     ]
    }
   ],
   "source": [
    "#Y = pd.get_dummies(data['sentiment']).values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "mlb = LabelEncoder()\n",
    "sentiment = data['Sentiment'].to_numpy()\n",
    "mlb.fit(sentiment)\n",
    "Y = mlb.transform(sentiment)\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "30095f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1799, 250) (1799,)\n",
      "(200, 250) (200,)\n"
     ]
    }
   ],
   "source": [
    "# DataSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, stratify=Y, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "58e8f2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3186 - accuracy: 0.6572 - val_loss: 0.6249 - val_accuracy: 0.7111\n",
      "Epoch 2/5\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6249 - accuracy: 0.6949 - val_loss: 0.6031 - val_accuracy: 0.7111\n",
      "Epoch 3/5\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6130 - accuracy: 0.6949 - val_loss: 0.6018 - val_accuracy: 0.7111\n",
      "Epoch 4/5\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5990 - accuracy: 0.6949 - val_loss: 0.6093 - val_accuracy: 0.7111\n",
      "Epoch 5/5\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5734 - accuracy: 0.6967 - val_loss: 0.5942 - val_accuracy: 0.7111\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding,SpatialDropout1D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8a60458f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 25ms/step - loss: 0.5922 - accuracy: 0.6950\n",
      "Test set\n",
      "  Loss: 0.592\n",
      "  Accuracy: 0.695\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4ca593",
   "metadata": {},
   "source": [
    "# CNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cae1e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49003d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "training_bs, test = train_test_split(data, \n",
    "                                         test_size=0.1, \n",
    "                                         random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a421db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_bs.loc[training_bs['Sentiment']=='positive', 'sentiment_score'] = int(1)\n",
    "training_bs.loc[training_bs['Sentiment']=='negative', 'sentiment_score'] = int(2)\n",
    "training_bs.loc[training_bs['Sentiment']=='neutral', 'sentiment_score'] = int(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b0d8373e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118129 words total, with a vocabulary size of 118\n"
     ]
    }
   ],
   "source": [
    "## build training vocabulary and get maximum training sentence length and total number of words training data\n",
    "all_training_words = ''.join([ word for tokens in training_bs[\"SentimentText\"] for word in tokens])\n",
    "training_sentence_lengths = [len(tokens) for tokens in training_bs[\"SentimentText\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c7775c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13021 words total, with a vocabulary size of 96\n",
      "Max sentence length is 148\n"
     ]
    }
   ],
   "source": [
    "## Build testing vocabulary and get maximum testing sentence length and total number of words in testing data\n",
    "\n",
    "all_test_words = ''.join([word for tokens in test[\"SentimentText\"] for word in tokens])\n",
    "test_sentence_lengths = [len(tokens) for tokens in test[\"SentimentText\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d95e22d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [....................................................................] 1647046227 / 1647046227"
     ]
    }
   ],
   "source": [
    "## Loading Google News Word2Vec model\n",
    "import wget\n",
    "url = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
    "filename = wget.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cd5dff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "071780ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dc82a984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5090 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer.fit_on_texts(training_bs[\"SentimentText\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(training_bs[\"SentimentText\"].tolist())\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d506a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "904a4e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "train_cnn_data = pad_sequences(training_sequences, \n",
    "                               maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f6377099",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding_weights = np.zeros((len(train_word_index)+1, \n",
    " EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "79000137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5091, 300)\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec word Embedding\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "612c9dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    " \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=300, \n",
    "                        kernel_size=filter_size, \n",
    "                        activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='softmax')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    preds = Dense(labels_index, activation='softmax')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5a988c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [1.0,2.0,3.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "44ce8506",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_cnn_data\n",
    "y_train = training_bs['sentiment_score'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9d40dea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 500, 300)     1527300     ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)             (None, 499, 300)     180300      ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)             (None, 498, 300)     270300      ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)             (None, 497, 300)     360300      ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)             (None, 496, 300)     450300      ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_24 (Conv1D)             (None, 495, 300)     540300      ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " global_max_pooling1d_20 (Globa  (None, 300)         0           ['conv1d_20[0][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " global_max_pooling1d_21 (Globa  (None, 300)         0           ['conv1d_21[0][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " global_max_pooling1d_22 (Globa  (None, 300)         0           ['conv1d_22[0][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " global_max_pooling1d_23 (Globa  (None, 300)         0           ['conv1d_23[0][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " global_max_pooling1d_24 (Globa  (None, 300)         0           ['conv1d_24[0][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 1500)         0           ['global_max_pooling1d_20[0][0]',\n",
      "                                                                  'global_max_pooling1d_21[0][0]',\n",
      "                                                                  'global_max_pooling1d_22[0][0]',\n",
      "                                                                  'global_max_pooling1d_23[0][0]',\n",
      "                                                                  'global_max_pooling1d_24[0][0]']\n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 1500)         0           ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 128)          192128      ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 128)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 3)            387         ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,521,315\n",
      "Trainable params: 1,994,015\n",
      "Non-trainable params: 1,527,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Function\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "model = ConvNet(train_embedding_weights, \n",
    "                MAX_SEQUENCE_LENGTH, \n",
    "                len(train_word_index)+1, \n",
    "                EMBEDDING_DIM, \n",
    "                len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "229482a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2243da43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2898, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2944, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3169, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3361, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3441, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\18604\\AppData\\Local\\Temp/ipykernel_21344/3461252470.py\", line 1, in <module>\n      hist = model.fit(x_train, y_train, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\losses.py\", line 245, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\losses.py\", line 1862, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\backend.py\", line 5202, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nReceived a label value of -9223372036854775808 which is outside the valid range of [0, 3).  Label values: -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_5330]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21344/3461252470.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2898, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2944, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3169, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3361, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3441, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\18604\\AppData\\Local\\Temp/ipykernel_21344/3461252470.py\", line 1, in <module>\n      hist = model.fit(x_train, y_train, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\losses.py\", line 245, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\losses.py\", line 1862, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\18604\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\backend.py\", line 5202, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nReceived a label value of -9223372036854775808 which is outside the valid range of [0, 3).  Label values: -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_5330]"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a382be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
